  OpenAI — Qflow Cloud          

*   [Qflow](https://qflowbpm.com/es/)
*   [Foro](https://forum.qflowbpm.com/)
*   [Centro de Ayuda](https://qflowbpm.com/es/centro-de-ayuda/)
*   [Contáctanos](https://qflowbpm.com/es/contacto/)

[Qflow](index.md)

Cloud (latest) 5.5 OnPremise (latest) 5.2 OnPremise 5.1.1 OnPremise

English Español

selectElement('versionSelect', getVersion()); selectElement('languageSelect', getLanguage()); function selectElement(id, valueToSelect) { let element = document.getElementById(id); element.value = valueToSelect; } function getLanguage() { if (window.location.href.includes('/es/')) { return '/es/'; } else { return '/en/'; } } function getVersion() { if (window.location.href.includes('/qflowcloud/')) { return '/qflowcloud/'; } else if (window.location.href.includes('/qflow5\_1\_1/')) { return '/qflow5\_1\_1/'; } else if (window.location.href.includes('/qflow5\_2/')) { return '/qflow5\_2/'; } else { return '/qflow5\_5/'; } } function redirectToSite(url) { var http = new XMLHttpRequest(); http.onreadystatechange = function() { if (http.readyState === 4) { if (http.status !== 404) { window.location.href = url; } else { window.location.href = url.replace(url.substr(url.lastIndexOf('/') + 1), 'index.md'); } } } http.open('HEAD', url, true); http.send(); }

  

Inicio

*   [Novedades](29-ReleaseNote.md)
*   [Introducción a Qflow](01-QflowIntroduction.md)
*   [Tutoriales](TutorialsIndex.md)
*   [Qflow Task](04-QflowTask.md)
*   [Qflow Design](15-QflowDesign.md)
*   [Qflow Team](18-QflowTeam.md)
*   [Qflow Admin](19-QflowAdmin.md)
*   [Consumo de Q-points](21-Q-pointsConsumption.md)
*   [Conectores](34-ConnectorsIndex.md)
    *   [Configuración de Conectores desde una Tarea de Servicio](35-ConnectorParameterConfig.md)
    *   [Microsoft Teams](36.1-MicrosoftTeamsConnector.md)
    *   [Outlook](36.2-OutlookConnector.md)
    *   [Slack](36.3-SlackConnector.md)
    *   [Trello](36.4-TrelloConnector.md)
    *   [OpenAI](#)
        *   [Introducción](#introduccion)
        *   [Prerrequisitos](#prerrequisitos)
        *   [Parámetro de aplicación](#parametro-de-aplicacion)
        *   [Acciones](#acciones)
    *   [Jira Cloud](36.6-JiraCloudConnector.md)
    *   [Redmine](36.7-RedmineConnector.md)
    *   [DocuSign](36.8-DocuSignConnector.md)
    *   [Google Calendar](36.9-GoogleCalendarConnector.md)
    *   [Dropbox](36.11-DropboxConnector.md)
    *   [OneDrive](36.12-OneDriveConnector.md)
    *   [WhatsApp](36.13-WhatsAppTwilioConnector.md)
    *   [Microsoft Word](36.14-MicrosoftWordConnector.md)
    *   [ILovePDF](36.15-ILovePDFConnector.md)
*   [Desarrolladores](31-Development.md)

[Qflow](index.md)

*   [](index.md)
*   [Conectores](34-ConnectorsIndex.md)
*   OpenAI

- - -

# OpenAI[](#openai "Link to this heading")

## Introducción[](#introduccion "Link to this heading")

El propósito de este manual es detallar cómo configurar e integrar Qflow con OpenAI para permitir la interacción entre ambas plataformas, permitiendo utilizar los servicios de OpenAI, como la moderación de contenido y ChatGPT, desde Qflow.

OpenAI cuenta con las siguientes acciones:

*   [Moderar contenido](#id1)
    
*   [Chat](#id2)
    

## Prerrequisitos[](#prerrequisitos "Link to this heading")

Es necesario contar con una cuenta de OpenAI. Si no tienes una, puedes crearla [aquí](https://platform.openai.com/signup).

## Parámetro de aplicación[](#parametro-de-aplicacion "Link to this heading")

Para entablar la conexión es necesario contar con al menos un **parámetro de aplicación** (ver [Parámetros de aplicación](15-QflowDesign.md#parametros-de-aplicacion)) que permita establecer la comunicación entre Qflow y OpenAI. Este parámetro puede ser creado desde la configuración de la tarea de servicio en Qflow, donde también se utiliza para establecer la comunicación entre Qflow y OpenAI. (ver [Configuración de Conectores desde una Tarea de Servicio](35-ConnectorParameterConfig.md))

Para crear un parámetro de aplicación de OpenAI, se deben seguir los pasos que se detallan a continuación.

### Parámetro de aplicación utilizando una API Key[](#parametro-de-aplicacion-utilizando-una-api-key "Link to this heading")

Este tipo de parámetro de aplicación requiere lo siguiente:

*   **API Key**: Es la clave con la cual Qflow se autenticará en OpenAI para poder realizar las acciones disponibles.
    

Para obtenerla se deben seguir los siguientes pasos:

1.  Dirigirse a la siguiente [página](https://platform.openai.com/api-keys), y de ser necesario iniciar sesión.
    
2.  Una vez allí, hacer clic en el botón Create new secret key.
    
    > [![_images/ApiKey1.png](_images/ApiKey1.png)](_images/ApiKey1.png)
    > 
    > Figura 819 Panel de control de API Keys de OpenAI[](#id3 "Link to this image")
    
3.  Se abrirá una ventana en la que se podrán configurar los datos de la API Key. En el campo Name se puede asignar un nombre a la API Key, y en el campo Permissions se puede seleccionar el nivel de permisos que tendrá la API Key. Una vez configurados los datos, hacer clic en el botón Create secret key.
    
    > [![_images/ApiKey2.png](_images/ApiKey2.png)](_images/ApiKey2.png)
    > 
    > Figura 820 Creación de una nueva API Key[](#id4 "Link to this image")
    
4.  Una vez creada la API Key, se mostrará una ventana con la clave generada. Allí es importante copiar la clave con el botón Copy y guardarla en un lugar seguro, ya que no se podrá visualizar nuevamente. Es este el valor que se debe utilizar para el parámetro de aplicación en Qflow.
    
    > [![_images/ApiKey3.png](_images/ApiKey3.png)](_images/ApiKey3.png)
    > 
    > Figura 821 API Key generada[](#id5 "Link to this image")
    

Una vez completados los pasos, será posible el uso de OpenAI con Qflow utilizando el parámetro de aplicación creado.

## Acciones[](#acciones "Link to this heading")

Se pueden realizar las siguientes acciones con OpenAI:

### Moderar contenido[](#id1 "Link to this heading")

Esta opción permite moderar contenido utilizando el servicio de moderación de OpenAI.

|     |     |
| --- | --- |Tabla 16 Entradas[](#id6 "Link to this table")  
| Entrada | Descripción |
| --- | --- |
| Texto | **Requerido.** Texto que se desea moderar. |

|     |     |
| --- | --- |Tabla 17 Salidas[](#id7 "Link to this table")  
| Salida | Descripción |
| --- | --- |
| Resultado de moderación | Devuelve “verdadero” si el texto fue marcado como contenido no apropiado, y “falso” si no lo fue. |
| Categorías moderadas | Devuelve un listado de las categorías en las que fue marcado el texto como contenido inapropiado, ordenados de mayor a menor puntuación, según que tan inapropiado fue considerado para esa categoría. Para ver todas las categorías posibles, ver la [documentación de OpenAI](https://platform.openai.com/docs/api-reference/moderations/object). |
| Puntuación de categorías | Devuelve un texto JSON con la puntuación de cada categoría en la que fue moderado el text. Un valor más alto indica que el texto fue considerado más inapropiado para esa categoría. Un ejemplo de salida sería el siguiente:<br><br> {<br>     "sexual": 1.2282071e-06,<br>     "hate": 0.010696256,<br>     "harassment": 0.29842457,<br>     "self-harm": 1.5236925e-08,<br>     "sexual/minors": 5.7246268e-08,<br>     "hate/threatening": 0.0060676364,<br>     "violence/graphic": 4.435014e-06,<br>     "self-harm/intent": 8.098441e-10,<br>     "self-harm/instructions": 2.8498655e-11,<br>     "harassment/threatening": 0.63055265,<br>     "violence": 0.99011886,<br> } |

### Chat[](#id2 "Link to this heading")

Esta opción permite interactuar con el servicio de ChatGPT de OpenAI. Permite generar respuestas a partir de un mensaje de un usuario y un modelo de ChatGPT. También se pueden enviar mensajes de sistema para dar contexto o instrucciones sobre lo que se espera en la respuesta y mantener un hilo de conversación a través de múltiples mensajes.

|     |     |
| --- | --- |Tabla 18 Entradas[](#id8 "Link to this table")  
| Entrada | Descripción |
| --- | --- |
| Modelo | **Requerido.** El modelo utilizado para generar la respuesta. Para conocer que modelos hay disponibles haga click [aquí](https://platform.openai.com/docs/models). |
| Formato de respuesta | El formato en el que se desea recibir la respuesta. Los formatos disponibles son: `Text` y `Json`. Por defecto, se utilizará el formato `Text`. |
| Prompt de sistema | Texto que se utilizará como prompt para generar la respuesta. Puede servir para dar contexto o presentar instructiones sobre lo que se espera en la respuesta. |
| Mensaje del usuario | **Requerido.** El mensaje que se desea enviar a ChatGPT para generar la respuesta. |
| Historial de mensajes en formato JSON | Historial de la conversación en formato JSON que se utilizará para generar la respuesta. Puede servir para dar contexto o mantener un hilo de conversación a través de múltiples mensajes. Es una lista de objetos JSON, donde cada objeto tiene dos campos: `role` y `content`. `role` puede ser `system`, `user` o `assistant`, y `content` es el mensaje correspondiente. Un ejemplo de entrada sería el siguiente:<br><br> \[<br>     {"role": "system", "content": "You are a helpful assistant."},<br>     {"role": "user", "content": "What is the capital of France?"},<br>     {"role": "assistant", "content": "The capital of France is Paris."},<br>     {"role": "user", "content": "What is its population?"}<br> \] |
| Penalización de frecuencia | Número entre -2.0 y 2.0. Los valores positivos penalizan los nuevos tokens en función de su frecuencia existente en el texto hasta el momento, disminuyendo la probabilidad de que el modelo repita la misma línea textualmente. [Más información](https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty). |
| Número máximo de tokens | El número máximo de tokens que se permitirán en la respuesta generada. Si no se especifica, se utilizará el máximo implícito del modelo. |
| Cantidad de respuestas | Número de respuestas que se desean generar. Por defecto, se generará una sola respuesta. Ten en cuenta que se te cobrará por el total de tokens generados en todas las respuestas. |
| Penalización por presencia | Número entre -2.0 y 2.0. Los valores positivos penalizan los nuevos tokens en función de si aparecen en el texto hasta el momento, aumentando la probabilidad del modelo de hablar sobre nuevos temas. [Más información](https://platform.openai.com/docs/api-reference/chat/create#chat-create-presence_penalty). |
| Temperatura | Número entre 0 y 2. Se recomienda utilizar este campo o “Top P”, pero no ambos. Valores más altos harán que la salida sea más aleatoria, mientras que valores más bajos la harán más enfocada y determinística. [Más información](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature). |
| Top P | Número entre 0 y 1. Es una alternativa al muestreo con temperatura, llamada muestreo de núcleo, donde el modelo considera los resultados de los tokens con la probabilidad de masa top\_p. Así que 0.1 significa que solo se consideran los tokens que comprenden el 10% superior de la masa de probabilidad. Se recomienda utilizar este campo o “temperatura”, pero no ambos. [Más información](https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p). |

|     |     |
| --- | --- |Tabla 19 Salidas[](#id9 "Link to this table")  
| Salida | Descripción |
| --- | --- |
| Respuestas | **Requerido.** Lista de respuestas generadas por ChatGPT. En caso de haberse solicitado más de una respuesta, se devolverán todas las respuestas generadas. |
| Historial de mensajes en formato JSON | Historial de la conversación en formato JSON con todos los mensajes, incluyendo los generados por ChatGPT. Este campo puede ser usado como entrada en una llamada futura para mantener un hilo de conversación. Por ejemplo, suponiendo que en la entrada se envió el historial de mensajes, un prompt de sistema y se solicitaron dos respuestas, la salida podría ser la siguiente:<br><br>1 \[<br>2     {"role": "system", "content": "You are a helpful assistant."},<br>3     {"role": "user", "content": "What is the capital of France?"},<br>4     {"role": "assistant", "content": "The capital of France is Paris."},<br>5     {"role": "system", "content": "You are a lying assistant."},<br>6     {"role": "user", "content": "What is the capital of France?"},<br>7     {"role": "assistant", "content": "The capital of France is Madrid."},<br>8     {"role": "assistant", "content": "The capital of France is London."}<br>9 \]<br><br>En el ejemplo anterior, se puede ver que el historial de mensajes incluye los mensajes pasados (de la línea 2 a la 4), el prompt de sistema (línea 5), el mensaje del usuario (línea 6) y las respuestas generadas por ChatGPT (líneas 7 y 8). Para ver más detalles sobre el formato de este campo, ver [más arriba](#formato-historial-chat). |
| Motivo de finalización | Motivo por el cual se detuvo la generación de respuestas. Puede ser `stop`, `length`, `content_filter`, `tool_calls` o `function_call`. [Más información](https://platform.openai.com/docs/api-reference/chat/object#chat/object-choices). |
| JSON de respuesta | Devuelve el JSON de respuesta tal cual lo devuelve OpenAI. Para más información, ver la [documentación de OpenAI](https://platform.openai.com/docs/api-reference/chat/object). |
| Uso total | Devuelve el uso total de tokens. Incluye tanto los tokens de entrada como los tokens generados en las respuestas. |

[Anterior](36.4-TrelloConnector.md "Trello") [Siguiente](36.6-JiraCloudConnector.md "Jira Cloud")

- - -

© Derechos de autor 2025, Urudata Software.

jQuery(function () { SphinxRtdTheme.Navigation.enable(true); }); window.dataLayer = window.dataLayer || \[\]; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-LMDS8S4B42', { 'anonymize\_ip': false, });
