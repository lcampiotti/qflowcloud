  OpenAI — Qflow Cloud          

*   [Qflow](https://qflowbpm.com/es/)
*   [Foro](https://forum.qflowbpm.com/)
*   [Centro de Ayuda](https://qflowbpm.com/es/centro-de-ayuda/)
*   [Contáctanos](https://qflowbpm.com/es/contacto/)

[Qflow](index.html)

Cloud (latest) 5.5 OnPremise (latest) 5.2 OnPremise 5.1.1 OnPremise

English Español

selectElement('versionSelect', getVersion()); selectElement('languageSelect', getLanguage()); function selectElement(id, valueToSelect) { let element = document.getElementById(id); element.value = valueToSelect; } function getLanguage() { if (window.location.href.includes('/es/')) { return '/es/'; } else { return '/en/'; } } function getVersion() { if (window.location.href.includes('/qflowcloud/')) { return '/qflowcloud/'; } else if (window.location.href.includes('/qflow5\_1\_1/')) { return '/qflow5\_1\_1/'; } else if (window.location.href.includes('/qflow5\_2/')) { return '/qflow5\_2/'; } else { return '/qflow5\_5/'; } } function redirectToSite(url) { var http = new XMLHttpRequest(); http.onreadystatechange = function() { if (http.readyState === 4) { if (http.status !== 404) { window.location.href = url; } else { window.location.href = url.replace(url.substr(url.lastIndexOf('/') + 1), 'index.html'); } } } http.open('HEAD', url, true); http.send(); }

  

Home

*   [News](29-ReleaseNote.html)
*   [Introduction to Qflow](01-QflowIntroduction.html)
*   [Tutorials](TutorialsIndex.html)
*   [Qflow Task](04-QflowTask.html)
*   [Qflow Design](15-QflowDesign.html)
*   [Qflow Team](18-QflowTeam.html)
*   [Qflow Admin](19-QflowAdmin.html)
*   [Q-points usage](21-Q-pointsConsumption.html)
*   [Connectors](34-ConnectorsIndex.html)
    *   [Configuring Connectors from a Service Task](35-ConnectorParameterConfig.html)
    *   [Microsoft Teams](36.1-MicrosoftTeamsConnector.html)
    *   [Outlook](36.2-OutlookConnector.html)
    *   [Slack](36.3-SlackConnector.html)
    *   [Trello](36.4-TrelloConnector.html)
    *   [OpenAI](#)
        *   [Introduction](#introduccion)
        *   [Prerequisites](#prerrequisitos)
        *   [Application parameter](#parametro-de-aplicacion)
        *   [Actions](#acciones)
    *   [Jira Cloud](36.6-JiraCloudConnector.html)
    *   [Redmine](36.7-RedmineConnector.html)
    *   [DocuSign](36.8-DocuSignConnector.html)
    *   [Google Calendar](36.9-GoogleCalendarConnector.html)
    *   [Dropbox](36.11-DropboxConnector.html)
    *   [OneDrive](36.12-OneDriveConnector.html)
    *   [WhatsApp](36.13-WhatsAppTwilioConnector.html)
    *   [Microsoft Word](36.14-MicrosoftWordConnector.html)
    *   [ILovePDF](36.15-ILovePDFConnector.html)
*   [Developers](31-Development.html)

[Qflow](index.html)

*   [](index.html)
*   [Connectors](34-ConnectorsIndex.html)
*   OpenAI

- - -

# OpenAI[](#openai "Link to this heading")

## Introduction[](#introduccion "Link to this heading")

The purpose of this manual is to detail how to configure and integrate Qflow with OpenAI to allow interaction between both platforms, allowing the use of OpenAI services, such as content moderation and ChatGPT, from Qflow.

OpenAI has the following actions:

*   [Moderate content](#id1)
    
*   [Chat](#id2)
    

## Prerequisites[](#prerrequisitos "Link to this heading")

You need to have an OpenAI account. If you don’t have one, you can create one [here](https://platform.openai.com/signup).

## Application parameter[](#parametro-de-aplicacion "Link to this heading")

To establish the connection, it is necessary to have at least one **application parameter** (see [Application Parameters](15-QflowDesign.html#parametros-de-aplicacion)) that allows communication between Qflow and OpenAI. This parameter can be created from the service task configuration in Qflow, where it is also used to establish communication between Qflow and OpenAI. (see [Connector Configuration from a Service Task](35-ConnectorParameterConfig.html))

To create an OpenAI application parameter, follow the steps detailed below.

### Application parameter using an API Key[](#parametro-de-aplicacion-utilizando-una-api-key "Link to this heading")

This type of application parameter requires the following:

*   **API Key**: It is the key with which Qflow will authenticate with OpenAI to be able to perform the available actions.
    

To obtain it, follow these steps:

1.  Go to the following [page](https://platform.openai.com/api-keys), and if necessary, log in.
    
2.  Once there, click on the Create new secret key button.
    
    > [![_images/ApiKey1.png](_images/ApiKey1.png)](_images/ApiKey1.png)
    > 
    > Fig. 819 OpenAI API Keys control panel[](#id3 "Link to this image")
    
3.  A window will open where you can configure the API Key data. In the Name field you can assign a name to the API Key, and in the Permissions field you can select the level of permissions that the API Key will have. Once the data is configured, click on the Create secret key button.
    
    > [![_images/ApiKey2.png](_images/ApiKey2.png)](_images/ApiKey2.png)
    > 
    > Fig. 820 New API Key creation[](#id4 "Link to this image")
    
4.  Once the API Key is created, a window will appear with the generated key. It is important to copy the key with the Copy button and save it in a safe place, as it will not be visible again. This is the value that must be used for the application parameter in Qflow.
    
    > [![_images/ApiKey3.png](_images/ApiKey3.png)](_images/ApiKey3.png)
    > 
    > Fig. 821 Generated API Key[](#id5 "Link to this image")
    

Once the steps are completed, it will be possible to use OpenAI with Qflow using the created application parameter.

## Actions[](#acciones "Link to this heading")

It is possible to perform the following actions with OpenAI:

### Moderate content[](#id1 "Link to this heading")

This option allows content moderation using the OpenAI moderation service.

|     |     |
| --- | --- |Table 16 Inputs[](#id6 "Link to this table")  
| Input | Description |
| --- | --- |
| Text | **Required.** Text to be moderated. |

|     |     |
| --- | --- |Table 17 Outputs[](#id7 "Link to this table")  
| Output | Description |
| --- | --- |
| Flagged | Returns ‘true’ if the text was flagged as inappropriate content, and ‘false’ if it was not. |
| Flagged categories | Returns a list of the categories for which the text was flagged as inappropriate content, sorted from highest to lowest score, according to how inappropriate it was considered for that category. To see all possible categories, go to [OpenAI's documentation](https://platform.openai.com/docs/api-reference/moderations/object). |
| Category scores | Returns a JSON text with the score of each category for which the text was moderated. A higher value indicates that the text was considered more inappropriate for that category. A possible output would be the following:<br><br> {<br>     "sexual": 1.2282071e-06,<br>     "hate": 0.010696256,<br>     "harassment": 0.29842457,<br>     "self-harm": 1.5236925e-08,<br>     "sexual/minors": 5.7246268e-08,<br>     "hate/threatening": 0.0060676364,<br>     "violence/graphic": 4.435014e-06,<br>     "self-harm/intent": 8.098441e-10,<br>     "self-harm/instructions": 2.8498655e-11,<br>     "harassment/threatening": 0.63055265,<br>     "violence": 0.99011886,<br> } |

### Chat[](#id2 "Link to this heading")

This option allows you to interact with OpenAI’s ChatGPT service. It allows you to generate responses from a user’s message. You can also send system messages to provide context or instructions on what is expected in the response and maintain a conversation thread across multiple messages.

|     |     |
| --- | --- |Table 18 Inputs[](#id8 "Link to this table")  
| Input | Description |
| --- | --- |
| Model | **Required.** The model used to generate the response. To find out which models are available click [here](https://platform.openai.com/docs/models). |
| Response format | The format in which you want to receive the response. The available formats are: `Text` and `Json`. By default, the `Text` format will be used. |
| System prompt | Text that will be used as a prompt to generate the response. It can be used to provide context or provide instructions on what is expected in the response. |
| User message | **Required.** The message you want to sent to ChatGPT to generate the response. |
| Chat history in JSON format | Chat history in JSON format that will be used to generate the response. It can be used to provide context or maintain a conversation thread across multiple messages. It is a list of JSON objects, where each object has two fields: `role` and `content`. `role` can be `system`, `user` or `assistant`, and `content` is the corresponding message. An example of a possible input would be the following:<br><br> \[<br>     {"role": "system", "content": "You are a helpful assistant."},<br>     {"role": "user", "content": "What is the capital of France?"},<br>     {"role": "assistant", "content": "The capital of France is Paris."},<br>     {"role": "user", "content": "What is its population?"}<br> \] |
| Frequency penalty | Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the probability that the model will repeat the same line verbatim. [More information](https://platform.openai.com/docs/api-reference/chat/create#chat-create-frequency_penalty). |
| Maximum number of tokens | The maximum number of tokens allowed in the generated response. If not specified, the implicit maximum of the model will be used. |
| Number of choices | Number of responses you want to generate. By default, a single response will be generated. Keep in mind that you will be charged for the total number of tokens generated in all responses. |
| Presence penalty | Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s probability of talking about new topics. [More information](https://platform.openai.com/docs/api-reference/chat/create#chat-create-presence_penalty). |
| Temperature | Number between 0 and 2. It is recommended to use this field or ‘Top P’, but not both. Higher values will make the output more random, while lower values will make it more focused and deterministic. [More information](https://platform.openai.com/docs/api-reference/chat/create#chat-create-temperature). |
| Top P | Number between 0 and 1. It is an alternative to temperature sampling, called core sampling, where the model considers the results of tokens with the top\_p mass probability. So 0.1 means that only the tokens that make up the top 10% of the probability mass are considered. It is recommended to use this field or ‘temperature’, but not both. [More information](https://platform.openai.com/docs/api-reference/chat/create#chat-create-top_p). |

|     |     |
| --- | --- |Table 19 Outputs[](#id9 "Link to this table")  
| Output | Description |
| --- | --- |
| Responses | **Required.** List of responses generated by ChatGPT. If more than one response has been requested, all the responses generated will be returned. |
| Chat history in JSON format | Chat history in JSON format with all messages, including those generated by ChatGPT. This field can be used as input in a future call to maintain a conversation thread. For example, assuming that the message history was sent in the input, a system prompt was sent, and two responses were requested, the output could be as follows:<br><br>1 \[<br>2     {"role": "system", "content": "You are a helpful assistant."},<br>3     {"role": "user", "content": "What is the capital of France?"},<br>4     {"role": "assistant", "content": "The capital of France is Paris."},<br>5     {"role": "system", "content": "You are a lying assistant."},<br>6     {"role": "user", "content": "What is the capital of France?"},<br>7     {"role": "assistant", "content": "The capital of France is Madrid."},<br>8     {"role": "assistant", "content": "The capital of France is London."}<br>9 \]<br><br>In the previous example, you can see that the message history includes past messages (from line 2 to 4), the system prompt (line 5), the user message (line 6) and the responses generated by ChatGPT (lines 7 and 8). For more details on the format of this field, see [above](#formato-historial-chat). |
| Finish reason | Reason why the response generation was stopped. It can be `stop`, `length`, `content_filter`, `tool_calls` or `function_call`. [More information](https://platform.openai.com/docs/api-reference/chat/object#chat/object-choices). |
| Response JSON | Returns the response JSON as returned by OpenAI. For more information, see [OpenAI's documentation](https://platform.openai.com/docs/api-reference/chat/object). |
| Total usage | Returns the total token usage. It includes both the input tokens and the tokens generated in the responses. |

[Previous](36.4-TrelloConnector.html "Trello") [Next](36.6-JiraCloudConnector.html "Jira Cloud")

- - -

© Copyright 2025, Urudata Software.

jQuery(function () { SphinxRtdTheme.Navigation.enable(true); }); window.dataLayer = window.dataLayer || \[\]; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-LMDS8S4B42', { 'anonymize\_ip': false, });